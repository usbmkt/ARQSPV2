#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV18 Enhanced v18.0 - Enhanced AI Manager
Gerenciador de IA com hierarquia OpenRouter: Gemini-2.0 Flash â†’ Gemini Direct e fallbacks robustos
ZERO SIMULAÃ‡ÃƒO - Apenas modelos reais funcionais
Com delay de 10s entre requisiÃ§Ãµes e rotaÃ§Ã£o inteligente de APIs
"""

import os
import logging
import asyncio
import json
import aiohttp
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
from dotenv import load_dotenv
import time
from pathlib import Path
from utils.advanced_credit_manager import advanced_credit_manager

# Carregar variÃ¡veis de ambiente do diretÃ³rio correto
env_path = Path(__file__).parent.parent.parent / '.env'
if env_path.exists():
    load_dotenv(env_path)
    logging.info(f"âœ… VariÃ¡veis de ambiente carregadas de: {env_path}")
else:
    # Fallback para buscar .env em diretÃ³rios pais
    current_dir = Path(__file__).parent
    for _ in range(5):  # Buscar atÃ© 5 nÃ­veis acima
        env_file = current_dir / '.env'
        if env_file.exists():
            load_dotenv(env_file)
            logging.info(f"âœ… VariÃ¡veis de ambiente carregadas de: {env_file}")
            break
        current_dir = current_dir.parent
    else:
        logging.warning("âš ï¸ Arquivo .env nÃ£o encontrado")
        load_dotenv()  # Fallback para comportamento padrÃ£o

logger = logging.getLogger(__name__)

class EnhancedAIManager:
    """Gerenciador de IA aprimorado com hierarquia OpenRouter e fallbacks"""

    def __init__(self):
        """Inicializa o gerenciador aprimorado com hierarquia OpenRouter"""
        # Carregar chaves OpenRouter
        self.openrouter_keys = self._load_openrouter_keys()
        self.current_key_index = 0
        
        # Carregar chaves Gemini para fallback
        self.gemini_keys = self._load_gemini_keys()
        self.current_gemini_key_index = 0
        
        # Configurar hierarquia de modelos
        self.model_hierarchy = [
            {
                'name': 'google/gemini-2.0-flash-exp:free',
                'provider': 'openrouter',
                'priority': 1,
                'max_tokens': 8000,
                'temperature': 0.7
            },
            {
                'name': 'qwen/qwen3-32b',
                'provider': 'groq',
                'priority': 2,
                'max_tokens': 4000,
                'temperature': 0.7
            },
            {
                'name': 'gemma-3-27b-it',
                'provider': 'fireworks',
                'priority': 3,
                'max_tokens': 4000,
                'temperature': 0.7
            },
            {
                'name': 'gpt-4o-mini',
                'provider': 'openai',
                'priority': 4,
                'max_tokens': 4000,
                'temperature': 0.7
            },
            {
                'name': 'gemini-2.0-flash-exp',
                'provider': 'gemini_direct',
                'priority': 5,
                'max_tokens': 4000,
                'temperature': 0.7
            }
        ]
        
        # Controle de rate limiting e delays
        self.last_request_time = 0
        self.request_delay = 10  # 10 segundos entre requisiÃ§Ãµes
        self.request_lock = asyncio.Lock()
        
        self.search_orchestrator = None
        
        # Importar search orchestrator se disponÃ­vel
        try:
            from .real_search_orchestrator import RealSearchOrchestrator
            self.search_orchestrator = RealSearchOrchestrator()
            logger.info("âœ… Search Orchestrator carregado")
        except ImportError:
            logger.warning("âš ï¸ Search Orchestrator nÃ£o disponÃ­vel")

        logger.info("ğŸ¤– Enhanced AI Manager inicializado com hierarquia Gemini-2.0 Flash â†’ Gemini Direct")
        logger.info(f"ğŸ”‘ {len(self.openrouter_keys)} chaves OpenRouter carregadas")
        logger.info(f"ğŸ”‘ {len(self.gemini_keys)} chaves Gemini carregadas")
        logger.info(f"â±ï¸ Delay configurado: {self.request_delay}s entre requisiÃ§Ãµes")
    
    def _load_openrouter_keys(self) -> List[str]:
        """Carrega mÃºltiplas chaves OpenRouter"""
        keys = []
        
        # Chave principal
        main_key = os.getenv('OPENROUTER_API_KEY')
        if main_key and main_key.strip():
            keys.append(main_key.strip())
            
        # Chaves numeradas
        for i in range(1, 6):
            key = os.getenv(f'OPENROUTER_API_KEY_{i}')
            if key and key.strip():
                keys.append(key.strip())
                
        logger.info(f"âœ… {len(keys)} chaves OpenRouter carregadas")
        return keys
    
    def _load_gemini_keys(self) -> List[str]:
        """Carrega mÃºltiplas chaves Gemini"""
        keys = []
        
        # Chave principal
        main_key = os.getenv('GEMINI_API_KEY')
        if main_key and main_key.strip():
            keys.append(main_key.strip())
            
        # Chaves numeradas
        for i in range(1, 4):
            key = os.getenv(f'GEMINI_API_KEY_{i}')
            if key and key.strip():
                keys.append(key.strip())
                
        logger.info(f"âœ… {len(keys)} chaves Gemini carregadas")
        return keys
    
    def _get_next_openrouter_key(self) -> Optional[str]:
        """ObtÃ©m prÃ³xima chave OpenRouter com rotaÃ§Ã£o"""
        if not self.openrouter_keys:
            return None
            
        key = self.openrouter_keys[self.current_key_index]
        self.current_key_index = (self.current_key_index + 1) % len(self.openrouter_keys)
        logger.info(f"ğŸ”„ Rotacionando para chave OpenRouter #{self.current_key_index + 1}/{len(self.openrouter_keys)}")
        return key
    
    def _get_next_gemini_key(self) -> Optional[str]:
        """ObtÃ©m prÃ³xima chave Gemini com rotaÃ§Ã£o"""
        if not self.gemini_keys:
            return None
            
        key = self.gemini_keys[self.current_gemini_key_index]
        self.current_gemini_key_index = (self.current_gemini_key_index + 1) % len(self.gemini_keys)
        logger.info(f"ğŸ”„ Rotacionando para chave Gemini #{self.current_gemini_key_index + 1}/{len(self.gemini_keys)}")
        return key

    async def _apply_rate_limit_delay(self):
        """Aplica delay de 10 segundos entre requisiÃ§Ãµes"""
        async with self.request_lock:
            current_time = time.time()
            time_since_last_request = current_time - self.last_request_time
            
            if time_since_last_request < self.request_delay:
                wait_time = self.request_delay - time_since_last_request
                logger.info(f"â±ï¸ Aguardando {wait_time:.2f}s antes da prÃ³xima requisiÃ§Ã£o...")
                await asyncio.sleep(wait_time)
            
            self.last_request_time = time.time()

    async def _generate_with_openrouter(
        self,
        prompt: str,
        model_name: str,
        max_tokens: int = 4000,
        temperature: float = 0.7,
        system_prompt: Optional[str] = None,
        max_retries: int = 3
    ) -> Optional[str]:
        """Gera conteÃºdo usando OpenRouter com rotaÃ§Ã£o de chaves, delay e retry logic aprimorado"""
        
        # Preparar mensagens
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # Tentar com todas as chaves disponÃ­veis
        for attempt in range(len(self.openrouter_keys)):
            # Aplicar delay antes de cada requisiÃ§Ã£o
            await self._apply_rate_limit_delay()
            
            api_key = self._get_next_openrouter_key()
            if not api_key:
                continue
            
            # Retry logic para cada chave
            for retry in range(max_retries):
                try:
                    # Delay adicional para retries
                    if retry > 0:
                        delay = min(2 ** retry, 30)  # Exponential backoff, max 30s
                        logger.info(f"â±ï¸ Aguardando {delay}s antes do retry {retry + 1}")
                        await asyncio.sleep(delay)
                    
                    headers = {
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json",
                        "HTTP-Referer": "https://github.com/joscarmao/v1800finalv2",
                        "X-Title": "ARQV18 Enhanced v18.0"
                    }
                    
                    payload = {
                        "model": model_name,
                        "messages": messages,
                        "max_tokens": max_tokens,
                        "temperature": temperature,
                        "stream": False
                    }
                    
                    logger.info(f"ğŸ“¤ OpenRouter ({model_name}) - Chave {attempt + 1}/{len(self.openrouter_keys)}, Retry {retry + 1}/{max_retries}")
                    
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            "https://openrouter.ai/api/v1/chat/completions",
                            headers=headers,
                            json=payload,
                            timeout=aiohttp.ClientTimeout(total=120)
                        ) as response:
                            
                            if response.status == 200:
                                result = await response.json()
                                if result.get("choices") and len(result["choices"]) > 0:
                                    content = result["choices"][0]["message"]["content"]
                                    logger.info(f"âœ… OpenRouter {model_name} sucesso (chave #{self.current_key_index}, retry {retry + 1})")
                                    return content
                                else:
                                    logger.warning(f"âš ï¸ OpenRouter resposta vazia")
                                    continue
                            elif response.status == 429:
                                # Rate limit - aguardar mais tempo
                                logger.warning(f"âš ï¸ Rate limit OpenRouter - aguardando 60s")
                                await asyncio.sleep(60)
                                continue
                            elif response.status in [500, 502, 503, 504]:
                                # Erro do servidor - retry
                                error_text = await response.text()
                                logger.warning(f"âš ï¸ Erro servidor OpenRouter {response.status}: {error_text[:100]}")
                                continue
                            else:
                                error_text = await response.text()
                                logger.warning(f"âš ï¸ OpenRouter key {attempt + 1} falhou: {response.status} - {error_text[:200]}")
                                break  # NÃ£o retry para outros erros
                                
                except asyncio.TimeoutError:
                    logger.warning(f"â±ï¸ Timeout OpenRouter key {attempt + 1}, retry {retry + 1}")
                    continue
                except Exception as e:
                    logger.warning(f"âš ï¸ Erro OpenRouter key {attempt + 1}, retry {retry + 1}: {str(e)[:100]}")
                    continue
            
            # Se chegou aqui, todos os retries falharam para esta chave
        
        logger.error(f"âŒ Todas as {len(self.openrouter_keys)} chaves OpenRouter falharam para {model_name}")
        return None
    
    async def _generate_with_gemini_direct(
        self,
        prompt: str,
        max_tokens: int = 4000,
        temperature: float = 0.7,
        system_prompt: Optional[str] = None
    ) -> Optional[str]:
        """Gera conteÃºdo usando Gemini direto com rotaÃ§Ã£o de chaves e delay"""
        
        try:
            import google.generativeai as genai
            
            # Tentar com todas as chaves Gemini
            for attempt in range(len(self.gemini_keys)):
                # Aplicar delay antes de cada requisiÃ§Ã£o
                await self._apply_rate_limit_delay()
                
                api_key = self._get_next_gemini_key()
                if not api_key:
                    continue
                    
                try:
                    genai.configure(api_key=api_key)
                    model = genai.GenerativeModel("gemini-2.0-flash-exp")
                    
                    # Combinar system prompt e user prompt se necessÃ¡rio
                    full_prompt = prompt
                    if system_prompt:
                        full_prompt = f"{system_prompt}\n\n{prompt}"
                    
                    generation_config = {
                        'temperature': temperature,
                        'top_p': 0.95,
                        'top_k': 64,
                        'max_output_tokens': max_tokens,
                    }
                    
                    logger.info(f"ğŸ“¤ Enviando requisiÃ§Ã£o para Gemini Direct - Tentativa {attempt + 1}/{len(self.gemini_keys)}")
                    
                    response = model.generate_content(
                        full_prompt,
                        generation_config=generation_config
                    )
                    
                    if response.text:
                        logger.info(f"âœ… Gemini direto sucesso (chave #{self.current_gemini_key_index})")
                        return response.text
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ Erro Gemini key {attempt + 1}: {str(e)[:100]}")
                    continue
            
            logger.error(f"âŒ Todas as {len(self.gemini_keys)} chaves Gemini falharam")
            return None
            
        except ImportError:
            logger.error("âŒ google-generativeai nÃ£o instalado")
            return None
    
    def generate_response(
        self,
        prompt: str,
        model: str = "google/gemini-2.0-flash-exp:free",
        max_tokens: int = 4000,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """Gera resposta sÃ­ncrona usando hierarquia de modelos"""
        try:
            # Executar geraÃ§Ã£o assÃ­ncrona de forma sÃ­ncrona
            import asyncio
            
            async def _async_generate():
                return await self.generate_text(
                    prompt=prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    model_override=model
                )
            
            # Tentar obter loop existente ou criar novo
            try:
                loop = asyncio.get_running_loop()
                # Se jÃ¡ hÃ¡ um loop rodando, criar task
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, _async_generate())
                    content = future.result(timeout=180)
            except RuntimeError:
                # Nenhum loop rodando, executar diretamente
                content = asyncio.run(_async_generate())
            
            return {
                'success': True,
                'content': content,
                'model': model,
                'provider': 'hierarchy',
                'tokens_used': len(content.split()) * 1.3  # Estimativa
            }
            
        except Exception as e:
            logger.error(f"âŒ Erro na geraÃ§Ã£o de resposta: {e}")
            return {
                'success': False,
                'content': 'Erro interno ao gerar resposta',
                'error': str(e)
            }

    async def generate_text(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        model_override: Optional[str] = None
    ) -> str:
        """
        Gera texto usando hierarquia de modelos: Gemini-2.0 Flash â†’ Gemini Direct
        Com delay de 10s entre requisiÃ§Ãµes e rotaÃ§Ã£o de APIs
        
        Args:
            prompt: Prompt do usuÃ¡rio
            system_prompt: Prompt do sistema (opcional)
            max_tokens: MÃ¡ximo de tokens (opcional)
            temperature: Temperatura (opcional)
            model_override: Modelo especÃ­fico (opcional)
        
        Returns:
            String com a resposta da IA
        """
        max_tokens = max_tokens or 4000
        temperature = temperature or 0.7
        
        # Se modelo especÃ­fico foi solicitado, tentar apenas ele
        if model_override:
            target_models = [m for m in self.model_hierarchy if m['name'] == model_override]
            if not target_models:
                # Se modelo nÃ£o encontrado, usar hierarquia normal
                target_models = self.model_hierarchy
        else:
            target_models = self.model_hierarchy
        
        # Tentar cada modelo na hierarquia
        for model_config in target_models:
            try:
                logger.info(f"ğŸ¤– Tentando {model_config['name']} ({model_config['provider']})")
                
                if model_config['provider'] == 'openrouter':
                    result = await self._generate_with_openrouter(
                        prompt=prompt,
                        model_name=model_config['name'],
                        max_tokens=min(max_tokens, model_config['max_tokens']),
                        temperature=temperature,
                        system_prompt=system_prompt
                    )
                    
                elif model_config['provider'] == 'gemini_direct':
                    result = await self._generate_with_gemini_direct(
                        prompt=prompt,
                        max_tokens=min(max_tokens, model_config['max_tokens']),
                        temperature=temperature,
                        system_prompt=system_prompt
                    )
                    
                elif model_config['provider'] in ['groq', 'fireworks', 'openai']:
                    # Usar enhanced_api_rotation_manager para outros providers
                    result = await self._generate_with_enhanced_api(
                        prompt=prompt,
                        model_name=model_config['name'],
                        provider=model_config['provider'],
                        max_tokens=min(max_tokens, model_config['max_tokens']),
                        temperature=temperature,
                        system_prompt=system_prompt
                    )
                else:
                    logger.warning(f"âš ï¸ Provider desconhecido: {model_config['provider']}")
                    continue
                
                if result:
                    logger.info(f"âœ… Sucesso com {model_config['name']}")
                    return result
                else:
                    logger.warning(f"âš ï¸ {model_config['name']} nÃ£o retornou resultado")
                    
            except Exception as e:
                logger.error(f"âŒ Erro com {model_config['name']}: {str(e)[:100]}")
                continue
        
        # Se todos os modelos falharam, usar fallback bÃ¡sico
        logger.error("âŒ Todos os modelos da hierarquia falharam")
        raise Exception("Todos os modelos de IA falharam. Verifique as configuraÃ§Ãµes das APIs.")
    
    async def _generate_with_enhanced_api(
        self,
        prompt: str,
        model_name: str,
        provider: str,
        max_tokens: int = 4000,
        temperature: float = 0.7,
        system_prompt: Optional[str] = None
    ) -> Optional[str]:
        """Gera texto usando enhanced_api_rotation_manager"""
        try:
            # Importar api_rotation_manager
            from services.enhanced_api_rotation_manager import api_rotation_manager
            
            # Preparar prompt completo
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"{system_prompt}\n\n{prompt}"
            
            # Usar api_rotation_manager para gerar
            result = await api_rotation_manager.generate_text(
                prompt=full_prompt,
                model=model_name,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            if result and result.strip():
                logger.info(f"âœ… {provider.upper()} ({model_name}) retornou resultado")
                return result.strip()
            else:
                logger.warning(f"âš ï¸ {provider.upper()} ({model_name}) nÃ£o retornou resultado")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Erro {provider.upper()} ({model_name}): {str(e)[:100]}")
            return None
    
    def generate_text_sync(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        model_override: Optional[str] = None
    ) -> str:
        """VersÃ£o sÃ­ncrona da geraÃ§Ã£o de texto"""
        try:
            import asyncio
            
            async def _async_wrapper():
                return await self.generate_text(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    model_override=model_override
                )
            
            try:
                loop = asyncio.get_running_loop()
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, _async_wrapper())
                    return future.result(timeout=180)
            except RuntimeError:
                return asyncio.run(_async_wrapper())
                
        except (ConnectionError, TimeoutError) as e:
            logger.error(f"âŒ Erro de conexÃ£o ao gerar texto (sync): {str(e)}")
            raise
        except (ValueError, KeyError) as e:
            logger.error(f"âŒ Erro de parÃ¢metros ao gerar texto (sync): {str(e)}")
            raise
        except Exception as e:
            logger.error(f"âŒ Erro inesperado ao gerar texto (sync): {str(e)}")
            raise

    async def _perform_smart_search(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """Realiza busca inteligente com fallbacks Serper â†’ Jina â†’ EXA"""
        
        if not self.search_orchestrator:
            logger.warning("âš ï¸ Search Orchestrator nÃ£o disponÃ­vel")
            return []
        
        try:
            # 1. Tentar Serper primeiro
            logger.info(f"ğŸ” Tentando busca Serper para: {query}")
            serper_results = await self.search_orchestrator.search_serper(query, max_results)
            
            if serper_results:
                logger.info(f"âœ… Serper retornou {len(serper_results)} resultados")
                return serper_results
            
            # 2. Fallback para Jina
            logger.info(f"ğŸ” Fallback: Tentando busca Jina para: {query}")
            jina_results = await self.search_orchestrator.search_jina(query, max_results)
            
            if jina_results:
                logger.info(f"âœ… Jina retornou {len(jina_results)} resultados")
                return jina_results
            
            # 3. Fallback para EXA
            logger.info(f"ğŸ” Fallback: Tentando busca EXA para: {query}")
            exa_results = await self.search_orchestrator.search_exa(query, max_results)
            
            if exa_results:
                logger.info(f"âœ… EXA retornou {len(exa_results)} resultados")
                return exa_results
            
            logger.warning("âš ï¸ Todos os serviÃ§os de busca falharam")
            return []
            
        except Exception as e:
            logger.error(f"âŒ Erro na busca inteligente: {e}")
            return []

    async def generate_with_active_search(
        self,
        prompt: str,
        context: str = "",
        session_id: str = None,
        max_search_iterations: int = 3,
        preferred_model: str = None,
        min_processing_time: int = 0
    ) -> str:
        """
        Gera conteÃºdo com busca ativa usando hierarquia Gemini-2.0 Flash â†’ Gemini Direct
        Com delay de 10s entre requisiÃ§Ãµes
        """
        logger.info(f"ğŸ” Iniciando geraÃ§Ã£o com busca ativa (modelo: {preferred_model or 'hierarquia'})")
        
        # Registrar tempo de inÃ­cio para garantir tempo mÃ­nimo
        start_time = datetime.now()

        # Realizar buscas complementares se necessÃ¡rio
        additional_context = ""
        if max_search_iterations > 0:
            # Extrair termos de busca do prompt
            search_queries = self._extract_search_terms(prompt)
            
            for i, query in enumerate(search_queries[:max_search_iterations]):
                logger.info(f"ğŸ” Busca {i+1}/{len(search_queries)}: {query}")
                search_results = await self._perform_smart_search(query, max_results=3)
                
                if search_results:
                    additional_context += f"\n\n=== DADOS DE BUSCA: {query} ===\n"
                    for result in search_results:
                        additional_context += f"- {result.get('title', 'Sem tÃ­tulo')}: {result.get('snippet', result.get('description', ''))}\n"

        # Prepara prompt com instruÃ§Ãµes de busca e contexto
        enhanced_prompt = f"""
{prompt}

CONTEXTO PRINCIPAL:
{context}

{additional_context if additional_context else ""}

INSTRUÃ‡Ã•ES ESPECIAIS:
- Analise o contexto fornecido detalhadamente
- Use os dados de busca complementares para enriquecer a anÃ¡lise
- Procure por estatÃ­sticas, tendÃªncias e casos reais
- ForneÃ§a insights profundos baseados nos dados disponÃ­veis
- Combine informaÃ§Ãµes de mÃºltiplas fontes para criar anÃ¡lise robusta

IMPORTANTE: Gere uma anÃ¡lise completa e profissional baseando-se em TODOS os dados fornecidos.
"""

        # Sistema prompt para busca ativa
        system_prompt = """VocÃª Ã© um especialista em anÃ¡lise de mercado e tendÃªncias digitais com acesso a dados em tempo real.
        Sua funÃ§Ã£o Ã© gerar anÃ¡lises profundas e insights valiosos baseados nos dados fornecidos.
        Sempre forneÃ§a informaÃ§Ãµes precisas, atualizadas e acionÃ¡veis.
        Combine dados de mÃºltiplas fontes para criar anÃ¡lises robustas e confiÃ¡veis."""

        try:
            # Usar modelo preferido ou hierarquia
            logger.info(f"ğŸ¤– Gerando com modelo: {preferred_model or 'hierarquia Gemini-2.0 Flash â†’ Gemini Direct'}")
            
            # Gerar resposta usando hierarquia
            response = await self.generate_text(
                prompt=enhanced_prompt,
                system_prompt=system_prompt,
                max_tokens=4000,
                temperature=0.7,
                model_override=preferred_model
            )
            
            # Garantir tempo mÃ­nimo de processamento se especificado
            if min_processing_time > 0:
                elapsed_time = (datetime.now() - start_time).total_seconds()
                if elapsed_time < min_processing_time:
                    remaining_time = min_processing_time - elapsed_time
                    logger.info(f"â±ï¸ Aguardando {remaining_time:.1f}s para completar tempo mÃ­nimo")
                    await asyncio.sleep(remaining_time)
            
            logger.info("âœ… GeraÃ§Ã£o com busca ativa concluÃ­da")
            return response
            
        except Exception as e:
            logger.error(f"âŒ Erro na geraÃ§Ã£o com busca ativa: {e}")
            # Fallback simples
            try:
                return await self.generate_text(enhanced_prompt, system_prompt)
            except Exception as e2:
                logger.error(f"âŒ Erro no fallback: {e2}")
                raise
    
    def _extract_search_terms(self, prompt: str) -> List[str]:
        """Extrai termos de busca relevantes do prompt"""
        # ImplementaÃ§Ã£o bÃ¡sica - pode ser melhorada
        search_terms = []
        
        # Buscar por palavras-chave comuns
        keywords = ['mercado', 'brasil', 'tendÃªncias', 'estatÃ­sticas', 'dados', 'anÃ¡lise']
        
        for keyword in keywords:
            if keyword in prompt.lower():
                # Extrair contexto ao redor da palavra-chave
                words = prompt.lower().split()
                for i, word in enumerate(words):
                    if keyword in word:
                        # Pegar 2 palavras antes e depois
                        start = max(0, i-2)
                        end = min(len(words), i+3)
                        search_term = ' '.join(words[start:end])
                        search_terms.append(search_term)
                        break
        
        # Se nÃ£o encontrou termos especÃ­ficos, usar primeiras palavras
        if not search_terms:
            words = prompt.split()[:5]
            search_terms.append(' '.join(words))
        
        return search_terms[:3]  # MÃ¡ximo 3 buscas

    async def generate_content(
        self,
        prompt: str,
        service_type: str = 'ai_generation',
        model: str = None,
        max_tokens: int = 4000,
        temperature: float = 0.7,
        system_prompt: str = None,
        **kwargs
    ) -> str:
        """
        MÃ©todo de compatibilidade para generate_content usado pelo enhanced_module_processor
        
        Args:
            prompt: Prompt do usuÃ¡rio
            service_type: Tipo de serviÃ§o (compatibilidade)
            model: Modelo especÃ­fico
            max_tokens: MÃ¡ximo de tokens
            temperature: Temperatura
            system_prompt: Prompt do sistema
            **kwargs: Argumentos adicionais
        
        Returns:
            String com a resposta da IA
        """
        try:
            logger.info(f"ğŸ”„ generate_content chamado - service_type: {service_type}, model: {model}")
            
            # Mapear service_type para modelo se necessÃ¡rio
            model_mapping = {
                'ai_generation': 'gpt-4o-mini',
                'qwen': 'qwen/qwen-2.5-72b-instruct',
                'gemini': 'google/gemini-2.0-flash-exp:free',
                'openrouter': 'google/gemini-2.0-flash-exp:free'
            }
            
            # Usar modelo especÃ­fico ou mapear do service_type
            target_model = model or model_mapping.get(service_type, 'google/gemini-2.0-flash-exp:free')
            
            # Aplicar delay entre requisiÃ§Ãµes para evitar rate limiting
            await self._apply_rate_limit_delay()
            
            # Gerar conteÃºdo usando hierarquia
            result = await self.generate_text(
                prompt=prompt,
                system_prompt=system_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                model_override=target_model
            )
            
            if not result:
                raise Exception(f"Modelo {target_model} nÃ£o retornou resultado")
            
            logger.info(f"âœ… generate_content sucesso com {target_model}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Erro em generate_content: {e}")
            
            # Fallback para modelo local se disponÃ­vel
            try:
                logger.info("ğŸ”„ Tentando fallback para modelo local...")
                from services.local_model_manager import local_model_manager
                
                if local_model_manager.is_model_loaded():
                    fallback_result = local_model_manager.generate_text(
                        prompt=prompt,
                        max_tokens=max_tokens,
                        temperature=temperature
                    )
                    
                    if fallback_result:
                        logger.info("âœ… Fallback modelo local sucesso")
                        return fallback_result
                        
            except Exception as fallback_error:
                logger.error(f"âŒ Fallback modelo local falhou: {fallback_error}")
            
            # Se tudo falhou, re-raise a exceÃ§Ã£o original
            raise Exception(f"generate_content falhou: {str(e)}")

    async def analyze_content(
        self,
        content: str,
        analysis_type: str = "comprehensive",
        target_audience: str = "general",
        model_preference: str = None
    ) -> str:
        """
        Analisa conteÃºdo usando hierarquia OpenRouter com delay
        
        Args:
            content: ConteÃºdo para anÃ¡lise
            analysis_type: Tipo de anÃ¡lise (comprehensive, viral, market, etc.)
            target_audience: PÃºblico-alvo
            model_preference: PreferÃªncia de modelo
        
        Returns:
            AnÃ¡lise detalhada do conteÃºdo
        """
        system_prompt = f"""VocÃª Ã© um especialista em anÃ¡lise de conteÃºdo digital e marketing.
        Sua funÃ§Ã£o Ã© analisar conteÃºdo de forma {analysis_type} para o pÃºblico {target_audience}.
        ForneÃ§a insights acionÃ¡veis, tendÃªncias identificadas e recomendaÃ§Ãµes estratÃ©gicas."""
        
        analysis_prompt = f"""
Analise o seguinte conteÃºdo de forma {analysis_type}:

CONTEÃšDO:
{content}

PÃšBLICO-ALVO: {target_audience}

FORNEÃ‡A:
1. AnÃ¡lise detalhada do conteÃºdo
2. Pontos fortes e fracos identificados
3. Potencial viral e engajamento
4. RecomendaÃ§Ãµes de melhoria
5. EstratÃ©gias de distribuiÃ§Ã£o
6. Insights de mercado relevantes

Seja especÃ­fico, prÃ¡tico e acionÃ¡vel em suas recomendaÃ§Ãµes.
"""
        
        try:
            return await self.generate_text(
                prompt=analysis_prompt,
                system_prompt=system_prompt,
                max_tokens=3000,
                temperature=0.7,
                model_override=model_preference
            )
        except Exception as e:
            logger.error(f"âŒ Erro na anÃ¡lise de conteÃºdo: {e}")
            raise

    async def generate_insights(
        self,
        data: Dict[str, Any],
        insight_type: str = "market_trends",
        depth: str = "deep"
    ) -> str:
        """
        Gera insights baseados em dados usando hierarquia OpenRouter com delay
        
        Args:
            data: Dados para anÃ¡lise
            insight_type: Tipo de insight desejado
            depth: Profundidade da anÃ¡lise (shallow, medium, deep)
        
        Returns:
            Insights gerados
        """
        system_prompt = f"""VocÃª Ã© um analista de dados especializado em {insight_type}.
        Sua funÃ§Ã£o Ã© gerar insights {depth} baseados nos dados fornecidos.
        Sempre forneÃ§a anÃ¡lises precisas, tendÃªncias identificadas e recomendaÃ§Ãµes acionÃ¡veis."""
        
        data_str = json.dumps(data, indent=2, ensure_ascii=False)
        
        insights_prompt = f"""
Analise os seguintes dados e gere insights {depth} sobre {insight_type}:

DADOS:
{data_str}

FORNEÃ‡A:
1. Principais tendÃªncias identificadas
2. PadrÃµes e correlaÃ§Ãµes importantes
3. Oportunidades de mercado
4. Riscos e desafios
5. RecomendaÃ§Ãµes estratÃ©gicas
6. PrevisÃµes baseadas nos dados

Seja especÃ­fico, use nÃºmeros quando relevante e forneÃ§a insights acionÃ¡veis.
"""
        
        try:
            return await self.generate_text(
                prompt=insights_prompt,
                system_prompt=system_prompt,
                max_tokens=4000,
                temperature=0.6
            )
        except Exception as e:
            logger.error(f"âŒ Erro na geraÃ§Ã£o de insights: {e}")
            raise

    def get_status(self) -> Dict[str, Any]:
        """Retorna status do gerenciador"""
        return {
            "openrouter_keys_count": len(self.openrouter_keys),
            "gemini_keys_count": len(self.gemini_keys),
            "current_openrouter_key_index": self.current_key_index,
            "current_gemini_key_index": self.current_gemini_key_index,
            "request_delay_seconds": self.request_delay,
            "last_request_time": self.last_request_time,
            "search_orchestrator_available": self.search_orchestrator is not None,
            "model_hierarchy": [m['name'] for m in self.model_hierarchy],
            "timestamp": datetime.now().isoformat()
        }

    def reset_failed_models(self):
        """Reseta Ã­ndices de rotaÃ§Ã£o de chaves"""
        self.current_key_index = 0
        self.current_gemini_key_index = 0
        self.last_request_time = 0
        logger.info("âœ… Ãndices de rotaÃ§Ã£o resetados")

# InstÃ¢ncia global para uso em todo o projeto
enhanced_ai_manager = EnhancedAIManager()

# FunÃ§Ãµes de conveniÃªncia para uso direto
async def generate_ai_text(
    prompt: str,
    system_prompt: Optional[str] = None,
    max_tokens: Optional[int] = None,
    temperature: Optional[float] = None,
    model_override: Optional[str] = None
) -> str:
    """FunÃ§Ã£o de conveniÃªncia para geraÃ§Ã£o de texto"""
    return await enhanced_ai_manager.generate_text(
        prompt=prompt,
        system_prompt=system_prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        model_override=model_override
    )

def generate_ai_text_sync(
    prompt: str,
    system_prompt: Optional[str] = None,
    max_tokens: Optional[int] = None,
    temperature: Optional[float] = None,
    model_override: Optional[str] = None
) -> str:
    """FunÃ§Ã£o de conveniÃªncia sÃ­ncrona para geraÃ§Ã£o de texto"""
    return enhanced_ai_manager.generate_text_sync(
        prompt=prompt,
        system_prompt=system_prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        model_override=model_override
    )

if __name__ == "__main__":
    # Teste bÃ¡sico
    async def test():
        try:
            manager = EnhancedAIManager()
            
            print("ğŸ§ª Testando geraÃ§Ã£o de texto com delay e rotaÃ§Ã£o de APIs...")
            print(f"â±ï¸ Delay configurado: {manager.request_delay}s entre requisiÃ§Ãµes")
            print(f"ğŸ”‘ Chaves OpenRouter disponÃ­veis: {len(manager.openrouter_keys)}")
            print(f"ğŸ”‘ Chaves Gemini disponÃ­veis: {len(manager.gemini_keys)}")
            print()
            
            # Teste 1: GeraÃ§Ã£o simples
            print("ğŸ“ Teste 1: GeraÃ§Ã£o de texto simples")
            response1 = await manager.generate_text(
                prompt="Explique brevemente o que Ã© inteligÃªncia artificial",
                system_prompt="VocÃª Ã© um especialista em tecnologia"
            )
            print(f"âœ… Resposta 1 (primeiros 200 chars): {response1[:200]}...")
            print()
            
            # Teste 2: Segunda requisiÃ§Ã£o (deve aguardar 10s)
            print("ğŸ“ Teste 2: Segunda requisiÃ§Ã£o (testando delay)")
            response2 = await manager.generate_text(
                prompt="O que Ã© machine learning?",
                system_prompt="VocÃª Ã© um especialista em IA"
            )
            print(f"âœ… Resposta 2 (primeiros 200 chars): {response2[:200]}...")
            print()
            
            # Teste 3: Status do gerenciador
            print("ğŸ“Š Status do gerenciador:")
            status = manager.get_status()
            print(json.dumps(status, indent=2, default=str, ensure_ascii=False))
            print()
            
            print("âœ… Todos os testes concluÃ­dos com sucesso!")
            
        except Exception as e:
            print(f"âŒ Erro no teste: {e}")
            import traceback
            traceback.print_exc()
    
    asyncio.run(test())
